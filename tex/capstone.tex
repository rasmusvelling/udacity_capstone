\title{Dimensionality Reduction \& Feature Selection}
\author{
	Rasmus Nisted Velling\\
	\emph{Machine Learning Engineer Nanodegree}\\
	Capstone Project \\
	Udacity}
\date{\today}

\documentclass[12pt]{article}
\usepackage[a4paper]{geometry}
\usepackage{cite}
\usepackage{graphicx}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}

\begin{document}
\maketitle


\begin{abstract}
This is the paper's abstract \ldots
\end{abstract}

\tableofcontents


\section{Definition}\label{definition}

\subsection{Background}

\subsubsection{Dimensionality reduction and feature selection}

Dimensionality reduction and feature selection is at the heart of applied statistics and data science. In it's essence, dimension reduction is the practice of taking $p$ available features (also known as dimensions) and reducing them to $p_{reduced} <= p$, so that we end up with fewer features than before. This can be done in many ways ~\cite{wiki:dimred}, and feature selection is, in short, one particular way of doing so, where one `cherry picks' a subset of features based on some criteria, often a statistical scoring like the Akaike information criterion or likewise ~\cite{wiki:feature}.

Among other types of dimensionality reduction methods commonly used are Principal Component Analysis (PCA) and Random Projections, which both are transformations of the underlying data, so that the new set of features are (often linear) combinations of the original inputs. This has both advantages, in that new, more informationally rich features can arise, and that the new dataset, with fewer features is easier to process; and disadvantages, in that the blurring of underlying features can make interpratability difficult and possible loss of information, of not done carefully.

There can be practical as well as scientific motivations for doing dimensionality reduction in general and/or feature selection in particular. Dimensionality reduction in general, especially when dealing with hundreds, thousands, or more, features is often born out of a shear practical motivation of wanting to speed up computational processing time or making data easier to comprehend and vizualize.

Feature selection in particular, when done right, will also yield the benefits of quicker processing and easier understnading, but has the additional benefits of providing very concrete information about what particular inputs are of (statistical) importance to the problem at hand.

\subsubsection{Making sense of mess}

In commercial as well as academic settings, it is often common to face statistical problems, where an abundance of features are given, without any clear indication of which are import, and which are not. `Throwing everything at the model', will result in something that takes hours, maybe days, to process, and possibly without any usefull output. Thus the first problem at hand becomes that of `boiling down' the data to something that can be processed and will deliver explanatory value ~\cite{mboostintro}.

And even if we could process all available inputs with decent speed and quality, a lot of times supplying insights to a business or academic research also necessitates providing interpretability of some sorts and not just a `black box'. So eventually we will also want to strive for the `most simple explanation' that still fits data reasonably well ~\cite{wiki:occam}

At the same time in many realworld settings, especially in businesses, a few number of data scientist and analyst are expected to solve problems across many domains like marketing and sales, operations, logistics, infrastructure, HR and many more, while at the same time being required to deliver results fairly quickly and of a decent standard. In these types of settings flexibility and genericity become important model and framework aspects, as the analysis and data science teams will want to deliver scalable and easily maintainable solutions to the problems posed by the surrounding business.

Thus it is of interest to explore models and frameworks which can help us deal with both feature selection and or dimensionality reduction in semi-automatic ways, as this will yield better models, speed up processing and save us precious time at work.

\subsubsection{NIPS 2003 Feature Selection Challenge}

The NIPS 2003 Feature Selection Challenge ~\cite{nips03workshop} mimicked the above-mentioned requirements into one competition, where participating teams were asked to come up with one model, or framework, which was then implemented across five datasets, sourced from different academic domains. The final score and ranking was then calculated as the average of the \emph{Balanced Error Rate} (defined in section \ref{model_val}) on each dataset, thus, requiring the teams to come up with flexible and generic models, that where somehow able to distinguish useable features from non-useable.

\subsection{Problem Overview}

Inspired by a specific, high dimensional, sparse data problem at work, I wanted to investigate flexible and broad methods for dealing with high dimensional, sparse classification problems. And while researching on this topic, I stumbled across the NIPS 2003 Challenge, which I in many ways feel reflects a lot of the problems I, as a Data Analyst, deal with in my day-to-day work.

In this Capstone Project I will explore model methods and frameworks inspired by the NIPS 2003 Feature Selection Challenge applied to the five datasets made available for the same challenge. A key component of the challenge was that the teams were scored on how well a \emph{single model type} would perform across all five datasets, thus requiring model frameworks to be flexible and generic. I will keep this dogma throughout this capstone as well.

Winning entries of the Challenge are described in the book Feature Extraction: Foundations and Applications ~\cite{nips03book} which came to life as a product of the best entries in the NIPS 2003 Challenge. Many of the winning entries are coded in Matlab or C, so this in this Capstone project I will focus on exploring how close one can come to the winning entries using readily available and popular tools in Python.


\subsection{Problem Statement}

\subsubsection{Problem}
How well do python-implementations (using readily available tools like sklearn) of “winning” models and other popular algorithms perform on the NIPS 2003 Challenge datasets?

\subsubsection{Solution}
\begin{itemize}
\item I will implement a series of classification models in python across the five datasets from the NIPS 2003 Feature Selection Challenge. 
\item These models will be inspired by the winning entries from the NIPS 2003 Feature Selection Challenge, described in ~\cite{nips03book}, as well as more recent, popular models like XGBOOST ~\cite{xgboostcite}, which has enjoyed a lot of success in challenges at kaggle.
\item I will apply the same models to all five datasets (as done in the original NIPS 2003 Challenge), to test the genericity and flexibility of the proposed solutions.
\item The models will be ranked on their \emph{Balanced Error Rate} score on the official validation datasets.
\end{itemize}

\subsection{Metrics}

The primary scoring metric will be the \emph{Balanced Error Rate} as used in the original NIPS 2003 Challenge ~\cite{nips03evaluation} and explained in section \ref{model_val}. The BER takes the unbalanced nature of one of the datasets into account, and also makes it easier to compare my own results with those of the wining entries. But I will also supply metrics such as \emph{f1 score} and \emph{area under curve} to the final rankings, so that models can be compared on these metrics as well.



\section{II. Analysis}
(approx. 2-4 pages)

\subsection{Data Exploration}

\subsubsection{Datasets}

The NIPS 2003 Feature Selection Challenge had five datasets. Originally the participants where given no information about the data other than that inputs had been scaled to integers between 0 and 999 for four of the five dataset, and that inputs where binary for the 5th dataset. And target variables where classified as either -1 or 1, making all the modelling problems binary classification type problems. Additionally all datasets had `false' features (called `probes' by the organizers) added, and the order of the features were randomized to make it more difficult to extract `good' features.

The five datasets vary widely in number of features and number of observations. The five datasets are:
\begin{description}
\item [ARCENE] Non sparse. Features: 10 000. Observations, training data: 100. Observations, validation data: 100. Arcene is based on mass-spectrometry data, and the underlying task is to separate cancer-patients from healthy patients. 
\item [DEXTER] Sparse Integer. Features: 20 000. Observations, training data: 300. Observations, validation data: 300. Dexter is based on Thorsten Joachims in the “bag-of-words” representation. The underlying task is to separate which Reuters articles are about `corporate acquisitions' and which are not.
\item[DOROTHEA] Sparse binary. Features: 100 000.  Observations, training data: 800.  Observations, validation data: 350. Dorothea is based on a drug-discovery dataset, and the task is to classify molecules as active (binding to thrombin) or inactive.
\item[GISETTE] Non sparse. Features: 5000. Observations, training data: 6000. Observations, validation data: 1000. Gisette is based on a handwritten digit dataset. The task is to separate 4 from 9.
\item[MADELON] Non sparse. Features: 500. Observations, training data: 2000. Observations, validation data: 600. Madelon is an artificially created dataset. The set contains 32 clusters of points based the vertices of a five dimensional hypercube and randomly labeled 1 or -1.
\end{description}

\begin{figure}[t]
  \begin{center}
    \includegraphics{fig__fig1.eps}
    \caption{\label{fig:sin_cos} Plot of the sine and cosine functions.}
  \end{center}
\end{figure}

In this section, you will be expected to analyze the data you are using for the problem. This data can either be in the form of a dataset (or datasets), input data (or input files), or even an environment. The type of data should be thoroughly described and, if possible, have basic statistics and information presented (such as discussion of input features or defining characteristics about the input or environment). Any abnormalities or interesting qualities about the data that may need to be addressed have been identified (such as features that need to be transformed or the possibility of outliers). Questions to ask yourself when writing this section:
- If a dataset is present for this problem, have you thoroughly discussed certain features about the dataset? Has a data sample been provided to the reader?
- If a dataset is present for this problem, are statistics about the dataset calculated and reported? Have any relevant results from this calculation been discussed?
- If a dataset is **not** present for this problem, has discussion been made about the input space or input data for your problem?
- Are there any abnormalities or characteristics about the input space or dataset that need to be addressed? (categorical variables, missing values, outliers, etc.)

\subsection{Exploratory Visualization}
In this section, you will need to provide some form of visualization that summarizes or extracts a relevant characteristic or feature about the data. The visualization should adequately support the data being used. Discuss why this visualization was chosen and how it is relevant. Questions to ask yourself when writing this section:
- Have you visualized a relevant characteristic or feature about the dataset or input data?
- Is the visualization thoroughly analyzed and discussed?
- If a plot is provided, are the axes, title, and datum clearly defined?

\subsection{Algorithms and Techniques}
In this section, you will need to discuss the algorithms and techniques you intend to use for solving the problem. You should justify the use of each one based on the characteristics of the problem and the problem domain. Questions to ask yourself when writing this section:
- Are the algorithms you will use, including any default variables/parameters in the project clearly defined?
- Are the techniques to be used thoroughly discussed and justified?
- Is it made clear how the input data or datasets will be handled by the algorithms and techniques chosen?

\subsection{Benchmark}
In this section, you will need to provide a clearly defined benchmark result or threshold for comparing across performances obtained by your solution. The reasoning behind the benchmark (in the case where it is not an established result) should be discussed. Questions to ask yourself when writing this section:
- Has some result or value been provided that acts as a benchmark for measuring performance?
- Is it clear how this result or value was obtained (whether by data or by hypothesis)?


\section{III. Methodology}
(approx. 3-5 pages)

\subsection{Data Preprocessing}
In this section, all of your preprocessing steps will need to be clearly documented, if any were necessary. From the previous section, any of the abnormalities or characteristics that you identified about the dataset will be addressed and corrected here. Questions to ask yourself when writing this section:
- If the algorithms chosen require preprocessing steps like feature selection or feature transformations, have they been properly documented?
- Based on the **Data Exploration** section, if there were abnormalities or characteristics that needed to be addressed, have they been properly corrected?
- If no preprocessing is needed, has it been made clear why?

\subsection{Implementation}
In this section, the process for which metrics, algorithms, and techniques that you implemented for the given data will need to be clearly documented. It should be abundantly clear how the implementation was carried out, and discussion should be made regarding any complications that occurred during this process. Questions to ask yourself when writing this section:
- Is it made clear how the algorithms and techniques were implemented with the given datasets or input data?
- Were there any complications with the original metrics or techniques that required changing prior to acquiring a solution?
- Was there any part of the coding process (e.g., writing complicated functions) that should be documented?

\subsection{Refinement}
In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. Questions to ask yourself when writing this section:
- Has an initial solution been found and clearly reported?
- Is the process of improvement clearly documented, such as what techniques were used?
- Are intermediate and final solutions clearly reported as the process is improved?


\section{IV. Results}
(approx. 2-3 pages)

\subsection{Model Evaluation and Validation}
\label{model_val}

\subsubsection{Balanced Error Rate}
In this section, the final model and any supporting qualities should be evaluated in detail. It should be clear how the final model was derived and why this model was chosen. In addition, some type of analysis should be used to validate the robustness of this model and its solution, such as manipulating the input data or environment to see how the model’s solution is affected (this is called sensitivity analysis). Questions to ask yourself when writing this section:
- Is the final model reasonable and aligning with solution expectations? Are the final parameters of the model appropriate?
- Has the final model been tested with various inputs to evaluate whether the model generalizes well to unseen data?
- Is the model robust enough for the problem? Do small perturbations (changes) in training data or the input space greatly affect the results?
- Can results found from the model be trusted?

\subsection{Justification}
In this section, your model’s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:
- Are the final results found stronger than the benchmark result reported earlier?
- Have you thoroughly analyzed and discussed the final solution?
- Is the final solution significant enough to have solved the problem?


\section{V. Conclusion}
(approx. 1-2 pages)

\subsection{Free-Form Visualization}
In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:
- Have you visualized a relevant or important quality about the problem, dataset, input data, or results?
- Is the visualization thoroughly analyzed and discussed?
- If a plot is provided, are the axes, title, and datum clearly defined?

\subsection{Reflection}
In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:
- Have you thoroughly summarized the entire process you used for this project?
- Were there any interesting aspects of the project?
- Were there any difficult aspects of the project?
- Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?

\subsection{Improvement}
In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:
- Are there further improvements that could be made on the algorithms or techniques you used in this project?
- Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?
- If you used your final solution as the new benchmark, do you think an even better solution exists?


\section{Before submitting, ask yourself. . .}

- Does the project report you’ve written follow a well-organized structure similar to that of the project template?
- Is each section (particularly **Analysis** and **Methodology**) written in a clear, concise and specific fashion? Are there any ambiguous terms or phrases that need clarification?
- Would the intended audience of your project be able to understand your analysis, methods, and results?
- Have you properly proof-read your project report to assure there are minimal grammatical and spelling mistakes?
- Are all the resources used for this project correctly cited and referenced?
- Is the code that implements your solution easily readable and properly commented?
- Does the code execute without error and produce results similar to those reported?

Blablabla said Nobody .

\bibliography{capstone}{}
\bibliographystyle{apalike}


\end{document}




